defaults:
  - _self_
  - env: env_config

name: DP-S_evaluation

rollout_mode: specific_ckpt
task_name: mtask_full_mask_one_encoder
run_name: "default"
data_root: ${env.data_root}
meta_path: ${env.meta_path}
job_id: 0

obj_classes: null
task_type: null
split: null

shape_meta:
  obs:
    agentview_rgb:
      shape: [3, 300, 300]
      type: rgb
    agentview_part_mask:
      shape: [1, 300, 300]
      type: mask
    gripper_state:
      shape: [1]
      type: low_dim
    joint_states:
      shape: [7]
      type: low_dim
    instructions:
      shape: [512]
      type: text
  action:
    shape: [7]

output_dir: "PartInstruct/outputs/DP-S"
starting_epoch: 0

seq_length: 16
horizon: 16
n_obs_steps: 2
n_action_steps: 8
obs_as_global_cond: true

num_iterations: 10
ckpt_path: null
epoch: 0
max_epoch: null

optimizer:
  _target_: torch.optim.AdamW
  lr: 1.0e-4
  betas: [0.95, 0.999]
  eps: 1.0e-8
  weight_decay: 1.0e-6

scheduler:
  _target_: null
  parameters: null

training:
  use_ema: True
  checkpoint_every: 50
  num_epochs: 3000

lang_encoder:
  _target_: PartInstruct.baselines.utils.encoders.T5Encoder
  pretrained_model_name_or_path: t5-small

task:
  name: ${task_name}
  env_runner:
    _target_: PartInstruct.baselines.evaluation.env_runner.dp_gpt_env_runner.GPTEnvRunner
    lang_encoder: ${lang_encoder}
    bullet_env: PartInstruct.PartGym.env.bullet_env_sam_gpt
    fps: 20
    start_seed: 100000
    max_steps: 250
    n_action_steps: ${n_action_steps}
    n_obs_steps: ${n_obs_steps}
    n_envs: 16
    n_vis: 16
    gui: false
    debug_output: "PartInstruct/outputs/debug_output"
    env_config: ${env}
    shape_meta: ${shape_meta}

policy:
  _target_: PartInstruct.baselines.policy.diffusion_policy.RobodiffUnetImagePolicy
  shape_meta: ${shape_meta}
  noise_scheduler:
    _target_: diffusers.schedulers.scheduling_ddpm.DDPMScheduler
    num_train_timesteps: 100
    beta_start: 0.0001
    beta_end: 0.02
    beta_schedule: squaredcos_cap_v2
    variance_type: fixed_small # Yilun's paper uses fixed_small_log instead, but easy to cause Nan
    clip_sample: true # required when predict_epsilon=false
    prediction_type: epsilon # or sample

  obs_encoder:
    _target_: PartInstruct.baselines.utils.encoders.VisLangObsImageEncoder
    shape_meta: ${shape_meta}
    image_encoder:
      _target_: PartInstruct.baselines.utils.encoders.get_resnet
      name: resnet18
      weights: null
      input_channels: 4
    lang_encoder: ${lang_encoder}
    resize_shape: null
    crop_shape: [76, 76]
    random_crop: true
    use_group_norm: true
    share_image_encoder: false
    imagenet_norm: true

  horizon: ${horizon}
  n_action_steps: ${eval:'${n_action_steps}'}
  n_obs_steps: ${n_obs_steps}
  num_inference_steps: 100
  obs_as_global_cond: ${obs_as_global_cond}
  diffusion_step_embed_dim: 128
  down_dims: [512, 1024, 2048]
  kernel_size: 5
  n_groups: 8
  cond_predict_scale: true

logging:
  project: partinstruct-DP-S
  resume: false
  mode: offline
  name: ${name}_${task_name}_${run_name}_${now:%Y.%m.%d-%H.%M.%S}
  tags: ["${name}", "${task_name}", "${run_name}"]
  id: null
  group: null

wandb_project: partinstruct-DP-S
folder: ./

hydra:
  run:
    dir: outputs/${name}_${task_name}/${run_name}

experiment_log: null
debug: false
