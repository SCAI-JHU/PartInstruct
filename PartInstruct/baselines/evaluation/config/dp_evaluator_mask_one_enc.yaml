defaults:
  - _self_
  - env: dp_env_config_mask

_target_: lgm_bc.evaluation.dp_evaluator.DPEvaluator

name: evaluation

rollout_mode: specific_ckpt

shape_meta: ${env.shape_meta}
task_name: mtask_full_mask_one_encoder
run_name: "default"

data_root: ${env.data_root}
meta_path: ${env.meta_path}
job_id: null

obj_classes: null
task_type: null
split: null

output_dir: "PartInstruct/outputs/"
starting_epoch: 0

seq_length: 16
horizon: 16
n_obs_steps: 1
n_action_steps: 8
obs_as_global_cond: true

num_iterations: 10
ckpt_path: "PartInstruct/data/checkpoints/diffusion_policy/latest.ckpt"
epoch: 0
max_epoch: null

optimizer:
  _target_: torch.optim.AdamW
  lr: 1.0e-4
  betas: [0.95, 0.999]
  eps: 1.0e-8
  weight_decay: 1.0e-6

scheduler:
  _target_: null
  parameters: null

lang_encoder:
  _target_: PartInstruct.baselines.utils.encoders.T5Encoder
  pretrained_model_name_or_path: t5-small

training:
  use_ema: True
  checkpoint_every: 50
  num_epochs: 3000

task:
  name: ${task_name}
  env_runner:
    _target_: PartInstruct.baselines.evaluation.env_runner.gpt_env_runner.GPTEnvRunner
    lang_encoder:
      _target_: PartInstruct.baselines.utils.encoders.T5Encoder
      pretrained_model_name_or_path: t5-small
    bullet_env: PartInstruct.PartGym.env.bullet_env_sam_gpt
    fps: 20
    start_seed: 100000
    max_steps: 250
    n_action_steps: ${n_action_steps}
    n_obs_steps: ${n_obs_steps}
    n_envs: 16
    n_vis: 16
    env_config: "PartInstruct/baselines/evaluation/config/env/${env.config_name}.yaml"
    gui: false
    debug_output: "PartInstruct/outputs/debug_output"

policy:
  _target_: PartInstruct.baselines.policy.diffusion_policy.RobodiffUnetImagePolicy

  shape_meta: ${shape_meta}

  noise_scheduler:
    _target_: diffusers.schedulers.scheduling_ddpm.DDPMScheduler
    num_train_timesteps: 100
    beta_start: 0.0001
    beta_end: 0.02
    beta_schedule: squaredcos_cap_v2
    variance_type: fixed_small # Yilun's paper uses fixed_small_log instead, but easy to cause Nan
    clip_sample: true # required when predict_epsilon=false
    prediction_type: epsilon # or sample

  obs_encoder:
    _target_: PartInstruct.baselines.utils.encoders.VisLangObsImageEncoder
    shape_meta: ${shape_meta}
    image_encoder:
      _target_: PartInstruct.baselines.utils.encoders.get_resnet
      name: resnet18
      weights: null
      input_channels: 4
    lang_encoder: ${lang_encoder}
    resize_shape: null
    crop_shape: [76, 76]
    # constant center crop
    random_crop: true
    use_group_norm: true
    share_image_encoder: false
    imagenet_norm: true

  horizon: ${horizon}
  n_action_steps: ${eval:'${n_action_steps}'}
  n_obs_steps: ${n_obs_steps}
  num_inference_steps: 100
  obs_as_global_cond: ${obs_as_global_cond}
  diffusion_step_embed_dim: 128
  down_dims: [512, 1024, 2048]
  kernel_size: 5
  n_groups: 8
  cond_predict_scale: true

logging:
  project: dp
  resume: false
  mode: offline
  name: ${name}_${task_name}_${run_name}_${now:%Y.%m.%d-%H.%M.%S}
  tags: ["${name}", "${task_name}", "${run_name}"]
  id: null
  group: null

pretrained_model_path: ""

wandb_project: lgm-robodiff_unet_image
folder: ./

hdf5_cache_mode: low_dim

hydra:
  run:
    dir: outputs/${name}_${task_name}/${run_name}

experiment_log: null
debug: false
